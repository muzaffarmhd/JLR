{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e78ad15",
   "metadata": {},
   "source": [
    "## JAX and Scientific Computing\n",
    "The basic prerequisite is multivariable calculus.\n",
    "\n",
    "### Multivariable Calculus: Review \n",
    "Multivariable functions can be scalar-valued or vector-valued, depending on whether their output is a scalar or a vector.\n",
    "\n",
    "#### 1. Scalar-valued functions:\n",
    "A scalar-valued function is used when you want a single number that measures something. In machine learning, this usually means a quantity you want to evaluate, compare, or optimize. The most important use of scalar-valued functions in machine learning is as loss (or cost) functions. A loss function takes many inputs—model parameters, predictions, and true labels—but outputs one number that tells you how bad the model is.\n",
    "\n",
    "#### 2. Vector-valued functions:\n",
    "A vector-valued function is used when the output naturally has multiple components. In machine learning, this usually means predictions, transformations, or feature mappings. Models produce structured outputs, not single numbers. Those outputs are vectors.\n",
    "\n",
    "Vector-valued functions are also used for:\n",
    "- Word embeddings (mapping a word to a vector)\n",
    "- Hidden layers in neural networks\n",
    "- Image feature representations\n",
    "\n",
    "$$\n",
    "\\mathbf{r}(t) = \\langle x(t),\\, y(t),\\, z(t) \\rangle\n",
    "$$\n",
    "\n",
    "Here, $x(t)$, $y(t)$, and $z(t)$ are ordinary real-valued functions, called the component functions. Together, they describe how each coordinate of the vector changes as $t$ changes.\n",
    "\n",
    "You can think of a vector-valued function as describing motion: as $t$ varies, the tip of the vector moves through space, tracing out a curve. The function tells you both magnitude and direction, which is why vector-valued functions are fundamental in physics and engineering.\n",
    "\n",
    "**Example:**\n",
    "$$\n",
    "\\mathbf{r}(t) = \\langle t,\\, t^2 \\rangle\n",
    "$$\n",
    "\n",
    "### Partial Derivatives: A New Perspective\n",
    "\n",
    "We might be familiar with partial derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x}, \\quad \\frac{\\partial f}{\\partial y}\n",
    "$$\n",
    "\n",
    "Each of these means:\n",
    "\n",
    "> \"What happens if I change one coordinate and freeze the others?\"\n",
    "\n",
    "But that is a very special kind of change.\n",
    "\n",
    "Now ask a more general question:\n",
    "\n",
    "> \"What if I move the input in some arbitrary direction?\"\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "f: \\mathbb{R}^n \\to \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Pick a direction vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} \\in \\mathbb{R}^n\n",
    "$$\n",
    "\n",
    "The directional derivative of $f$ at $\\mathbf{x}$ in direction $\\mathbf{v}$ is:\n",
    "\n",
    "$$\n",
    "D_{\\mathbf{v}} f(\\mathbf{x}) = \\lim_{\\epsilon \\to 0} \\frac{f(\\mathbf{x} + \\epsilon \\mathbf{v}) - f(\\mathbf{x})}{\\epsilon}\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "> \"If I nudge the input slightly in direction $\\mathbf{v}$, how fast does the output change?\"\n",
    "\n",
    "This is the most general first-derivative question you can ask.\n",
    "\n",
    "If a function is differentiable, then all directional derivatives are given by dot products with one fixed vector. That vector is the gradient, and if we want to find the vector that maximizes the change, then it is the fixed vector itself that we dot with, which is the gradient.\n",
    "\n",
    "$$\n",
    "D_{\\mathbf{v}} f(\\mathbf{x}) = \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n",
    "$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
